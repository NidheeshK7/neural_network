{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9152447,"sourceType":"datasetVersion","datasetId":5528765},{"sourceId":1313369,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd \n\nclass Linear:\n    def __init__(self,in_features,out_features):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weights = np.random.randn(in_features, out_features) * 0.01\n        self.biases = np.zeros((1, out_features))\n        self.input = None\n        self.grad_weights = None\n        self.grad_biases = None\n    def forward(self,x):\n        self.input = x\n        return np.dot(x, self.weights) + self.biases\n    def backward(self,grad_output):\n        self.grad_weights = np.dot(self.input.T, grad_output)\n        self.grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n        return np.dot(grad_output, self.weights.T)        \n\n\nclass ReLU:\n    def forward(self,x):\n        self.input = x\n        return np.maximum(0, x)\n    def backward(self,grad_output):\n        grad_input = grad_output.copy()\n        grad_input[self.input <= 0] = 0\n        return grad_input\n\nclass Sigmoid:\n    def forward(self,x):\n        self.input = x\n        return 1 / (1 + np.exp(-x))\n    def backward(self,grad_output):\n        sigmoid = 1 / (1 + np.exp(-self.input))\n        return grad_output * sigmoid * (1 - sigmoid)\n\nclass Tanh:\n    def forward(self,x):\n        self.input = x\n        return np.tanh(x)\n    def backward(self,grad_output):\n        return grad_output * (1 - np.tanh(self.input) ** 2)\n\nclass Softmax:\n    def forward(self,x):\n        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n    def backward(self,grad_output):\n        return grad_output\n\nclass CrossEntropyLoss:\n    def forward(self,predictions,targets):\n        m = predictions.shape[0]\n        p = Softmax().forward(predictions)\n        log_likelihood = -np.log(p[range(m), targets])\n        loss = np.sum(log_likelihood) / m\n        return loss\n    def backward(self,predictions,targets):\n        m = predictions.shape[0]\n        grad = Softmax().forward(predictions)\n        grad[range(m), targets] -= 1\n        grad = grad / m\n        return grad\n\nclass MSELoss:\n    def forward(self,predictions,targets):\n        return np.mean((predictions - targets) ** 2)\n    def backward(self,predictions,targets):\n        return 2 * (predictions - targets) / targets.size\n\nclass SGD:\n    def __init__(self,parameters, learning_rate=0.01):\n        self.parameters = parameters\n        self.learning_rate = learning_rate\n    def step(self):\n        for param in self.parameters:\n            param['weight'] -= self.learning_rate * param['grad_weight']\n            param['bias'] -= self.learning_rate * param['grad_bias']\n\nclass Model:\n    def __init__(self):\n        self.layers = []\n        self.loss_fn = None\n        self.optimizer = None\n    \n    def add_layer(self, layer):\n        self.layers.append(layer)\n    \n    def compile(self, loss, optimizer):\n        self.loss_fn = loss\n        self.optimizer = optimizer\n\n    def train(self, x_train, y_train, epochs=20):\n        for epoch in range(epochs):\n            # Forward pass\n            predictions = x_train\n            for layer in self.layers:\n                predictions = layer.forward(predictions)\n\n            # Compute loss\n            loss = self.loss_fn.forward(predictions, y_train)\n            print(f'Epoch {epoch + 1}, Loss: {loss}')\n\n            # Backward pass\n            self.loss_fn.backward(predictions, y_train)\n            for layer in reversed(self.layers):\n                layer.backward()\n\n            # Update parameters\n            self.optimizer.step()\n\n\n    def predict(self,x):\n        output = x\n        for layer in self.layers:\n            output = layer.forward(output)\n        return output\n    def evaluate(self, x_test, y_test):\n        predictions = self.predict(x_test)\n        loss = self.loss_fn.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == y_test)\n        print(f'Loss: {loss}, Accuracy: {accuracy}')\n        return loss, accuracy\n    def save(self,path):\n        model_params = {\n            \"layers\": self.layers,\n            \"optimizer\": self.optimizer\n        }\n        np.save(path, model_params)\n    def load(self,path):\n        model_params = np.load(path, allow_pickle=True).item()\n        self.layers = model_params['layers']\n        self.optimizer = model_params['optimizer']        \n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T16:46:52.319583Z","iopub.execute_input":"2024-08-12T16:46:52.319959Z","iopub.status.idle":"2024-08-12T16:46:52.352219Z","shell.execute_reply.started":"2024-08-12T16:46:52.319931Z","shell.execute_reply":"2024-08-12T16:46:52.351101Z"},"trusted":true},"execution_count":9,"outputs":[]}]}